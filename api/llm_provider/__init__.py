import os
from typing import List, Dict
import ollama


class LLMProvider:
    def __init__(self, provider: str = "ollama", model_name: str = "llama3.1:latest"):
        self.provider = provider
        self.model_name = model_name
        print(f"Model Provider Selected: {self.provider}, Model Name: {self.model_name}")
        if self.provider == "ollama":
            models_names = list(map(lambda x : x["name"],ollama.list()["models"]))
            if self.model_name not in models_names:
                print("ollama pulling")
                ollama.pull(self.model_name)
            else:
                print("model exists")

    def get_response(self, system_prompt: str, prompt: str) -> (str, str):
        """
        Generates responses from the original LLM and the ShieldNet model.

        :param system_prompt: The system's instruction or context for the LLM.
        :param prompt: The user's input query or message.
        :return: A tuple containing responses from the original LLM and the ShieldNet.
        """
        original_response = self._get_original_response(system_prompt, prompt)
        shield_net_response = self._get_shield_net_response(system_prompt, prompt)
        return original_response, shield_net_response

    def _get_original_response(self, system_prompt: str, prompt: str) -> str:
        """
        Generates the response using the original LLM.

        :param system_prompt: The system's instruction or context for the LLM.
        :param prompt: The user's input query or message.
        :return: The response content from the original LLM.
        """
        messages = self._prepare_messages(system_prompt, prompt)
        return self._invoke_llm(messages)

    def _invoke_llm(self, messages: List[Dict]) -> str:
        """
        Invokes the LLM to generate a response.

        :param messages: A list of messages to provide context and user input.
        :return: The generated response content.
        """

        if self.provider == "ollama":
            response = ollama.chat(model=self.model_name,options={'temperature' : 0,'num_predict': 500}, messages=messages)
            return response.get("message", {}).get("content", "")
        raise NotImplementedError(f"Provider '{self.provider}' is not implemented")


    def _get_shield_net_response(self, system_prompt: str, prompt: str) -> str:
        """
        Generates the response using ShieldNet for safety verification and post-processing.

        :param system_prompt: The system's instruction or context for the LLM.
        :param prompt: The user's input query or message.
        :return: The ShieldNet-processed response or a safety warning.
        """
        if not (self._is_safe_prompt(system_prompt) and self._is_safe_prompt(prompt)):
            return "ShieldNet Model Detection - Not Safe"

        messages = self._prepare_messages(system_prompt, prompt)
        content = self._invoke_llm(messages)
        return self._post_process_content(content)

    def _prepare_messages(self, system_prompt: str, prompt: str) -> List[Dict]:
        """
        Prepares a list of messages for the LLM based on the system prompt and user input.

        :param system_prompt: The system's instruction or context for the LLM.
        :param prompt: The user's input query or message.
        :return: A list of formatted messages.
        """
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if prompt:
            messages.append({"role": "user", "content": prompt})
        return messages

    ## TODO
    def _post_process_content(self, content: str) -> str:
        """
        Applies post-processing to the LLM's response content.

        :param content: The raw content generated by the LLM.
        :return: The processed content.
        """
        # Add custom post-processing logic here if needed
        return content

    ## TODO
    def _is_safe_prompt(self, prompt: str) -> bool:
        """
        Checks if a given prompt is safe to process.

        :param prompt: The prompt to evaluate.
        :return: True if the prompt is safe, False otherwise.
        """
        # Add custom safety checks here
        return False


LLM_PROVIDER = os.environ.get("LLM_PROVIDER")
LLM_NAME = os.environ.get("LLM_NAME")

llm_provider = LLMProvider(provider=LLM_PROVIDER, model_name=LLM_NAME)
